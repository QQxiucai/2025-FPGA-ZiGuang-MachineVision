# 2025-FPGA-ZiGuang-MachineVision
上位机系统
## 作品速览 (详细技术阐述)：“智慧Vision”核心架构与算法解析

我们的作品“智慧Vision”是一套基于**紫光同创RK3568_MES2L50H平台**的“边-云协同”异构智慧视觉系统。


## 第一部分：作品概述与核心功能

### 1.1 项目目标：破解“软硬协同”难题

本项目围绕大赛核心要求，旨在设计并实现一套真正意义上的“软硬协同”视觉系统。我们面临的核心挑战是：如何在资源受限的边缘计算平台上，高效处理多路视频流，并实时运行复杂的AI算法，同时保证系统的低延迟、高可靠性和灵活性。

为此，我们拒绝了简单的功能堆叠，而是深度实践了从**硬件加速**、**异构推理**到**云端联动**的完整“边-云协同”架构，将RK3568_MES2L50H平台的潜力挖掘到极致。

### 1.2 核心功能：端到端的智能闭环

本系统实现了一个从“硬件采集”到“云端响应”的完整智能闭环，其功能可分解为以下四个关键阶段：

- **1. 硬件加速 (FPGA-ISP流水线):**
  - **并行采集：** 充分利用FPGA (PG2L50H) 的并行处理能力，通过FMC扩展板及40pin管脚驱动三路OV5640摄像头并行采集720p原始数据。
  - **片上ISP：** 数据流在FPGA片上实时通过我们构建的ISP（图像信号处理）流水线，即时完成8-to-16bit转换、灰度化及**3x3中值滤波**。
  - **核心价值：** 此“硬”预处理设计，**将ARM CPU和NPU从繁重的像素级运算中彻底解放出来**，使其能专注于AI推理等上层任务。
- **2. 异构推理 (ARM-SoC算力压榨):**
  - **高速传输：** 预处理后的数据经AXI总线存入DDR3帧缓存，最后由DMA引擎打包、通过PCIe IP核高速传输至ARM处理器（RK3568）。
  - **精细化调度：** 我们基于C++/Qt 5构建了一个高稳定性的多线程应用，**精细化调度NPU (1.0 TOPS)、CPU (A55) 和 RGA（2D图形加速器）硬件**：
    - **NPU (神经网络处理单元)：** 作为主力，高效执行YOLOv5、YOLOv6、Mobilenet等`int8`量化AI模型。
    - **CPU (中央处理器)：** 运行OpenCV DNN模块作为“纯软”路径对比；同时负责所有AI模型的后处理（如NMS）和跟踪器逻辑。
    - **RGA (图形加速器)：** 辅助NPU进行硬件色彩空间转换、缩放和Letterbox填充，再次为CPU和NPU减负。
- **3. 边-云协同 (实时告警闭环):**
  - **边缘决策：** 系统在**边缘端（本地）**实时完成所有AI分析。这种设计保证了即使在网络中断或不佳的情况下，核心安防功能依然7x24小时在线。
  - **异步“云”响应：** 当检测到“陌生人长期停留”等预设异常事件时，独立的上传线程被激活，立即抓拍快照并**异步上传**至**七牛云KODO**对象存储进行安全归档，同时调用**“喵提醒”API**向用户推送实时告警信息。
- **4. 动态交互 (QML人机界面):**
  - **模型切换：** 用户可通过UI界面，在YOLOv5、YOLOv6、Mobilenet和CPU纯软推理之间**动态切换**，展示了系统的多功能性。
  - **实时调参：** 用户可**实时调节**算法的置信度、NMS阈值、报警时间、MobileNet Top-K等关键参数，并即时在推理画面上看到结果，极大提升了系统的实用性和调试效率。

## 第二部分：关键创新与算法解析

本项目的核心创新在于“协同”二字，我们不仅实现了功能，更在“效率”和“灵活性”上做了深度优化。

### 2.1 关键创新与优越性

- **1. 深度“软硬协同”与异构压榨：** 本项目的最大优越性在于**深度的“软硬协同”**。我们没有简单地将FPGA用作一个PCIe网卡，而是将其打造成一个**主动式ISP处理器**。通过FPGA片上ISP流水线（“硬”）为后续的RGA、NPU（“硬”）和CPU（“软”）减负，实现了精细化的三级异构任务调度，将SoC的每一分算力都用在刀刃上。
- **2. “边缘决策、云端响应”的健壮架构：** 我们的架构兼顾了**低延迟**与**高可靠性**。AI决策在边缘实时完成（< 50ms），不依赖网络带宽；云端告警则采用异步上传，即使网络阻塞，也**绝不会卡顿**本地的AI推理线程。这使其在网络不佳的工业、仓储等场景下依然能7x24小时稳定工作。
- **3. 高灵活性与强实用性的算法调度：** 系统支持多模型动态切换和参数实时调优。这使其能灵活适配不同场景：需要高精度的安防场景可使用YOLOv6；需要多目标识别的场景用YOLOv5；仅需判断是否存在的场景可用Mobilenet。这种灵活性极大提升了方案的实用价值。

### 2.2 算法多样性分析

我们选择并部署了多种算法，其目的在于展示系统的**多功能性**和**异构路径对比**：

- **YOLOv5 (NPU路径):** 作为业界成熟的基准，它在速度和精度上取得了良好平衡。我们使用NPU执行其`int8`量化模型，是系统的**主力检测模型**。
- **YOLOv6 (NPU路径):** 作为更先进的检测模型，它引入了DFL（Distribution Focal Loss）等新技术。部署它证明了我们的系统具备**跟进前沿算法的能力**，并且我们的后处理逻辑能兼容不同（YOLOv5 vs v6）的输出张量。
- **Mobilenet (NPU路径):** 这是一个**图像分类**模型。我们引入它的战略意义在于证明本系统**不局限于“目标检测”**，而是一个通用的“智慧视觉平台”，可以随时切换到分类、分割等不同AI任务。
- **YOLOv5 (CPU路径):** 这是一个“纯软”的**性能对照组**。我们使用OpenCV DNN模块在ARM A55 CPU上运行未经优化的ONNX模型。其帧率远低于NPU路径，这**反向证明了NPU硬件加速的绝对必要性**，也凸显了“软硬协同”设计的巨大性能优势。

### 2.3 关键算法复杂度分析

“复杂度”是衡量嵌入式系统性能的关键。我们从“计算复杂度”和“时间复杂度”两个维度，对系统关键路径进行了分析和优化：

- **1. 硬件ISP (FPGA) 复杂度：O(1) / 像素**
  - **挑战：** 图像预处理（如灰度化、滤波）是像素密集型计算。以`3x3中值滤波`为例，在CPU上，朴素实现的时间复杂度为 `O(W * H * k^2)` (W=宽, H=高, k=3)。这是一个巨大的计算负担。
  - **我们的方案：** 我们在FPGA中使用Verilog构建了一个**深度流水线（Pipeline）**。
    - **灰度化：** `O(1)`的组合逻辑，在数据流经时瞬时完成。
    - **3x3中值滤波：** 我们使用了**行缓存 (Line Buffer)** 和一个**9输入排序网络**。当数据流经过时，系统始终维持一个3x3的滑动窗口。
  - **复杂度分析：** 在FPGA的流水线架构下，虽然存在几（例如7）个时钟周期的**固定延迟(Latency)**，但其**吞吐率(Throughput)可以达到1个像素/时钟周期**。这意味着，对于W*H的图像，其**时间复杂度被硬件优化为 O(W \* H)**，即**O(N)**（N为总像素），且其计算**完全不占用CPU/NPU资源**。这是典型的以硬件资源换取O(1)每像素处理时间的“硬”协同思路。
- **2. AI推理 (NPU/CPU) 复杂度：**
  - **NPU (1.0 TOPS)：** 我们的NPU拥有 1.0 TOPS 算力，即每秒可执行1万亿次`int8`定点运算。AI模型的计算复杂度通常用**FLOPs**（浮点运算次数）或**MACs**（乘加运算次数）衡量。
  - **复杂度优化 (量化)：** 原始的YOLOv5 (FP32) 模型被我们使用RKNPU工具转换为`int8`模型。此操作将模型大小和内存带宽需求降低了约4倍，并将计算复杂度从浮点运算（高消耗）转为NPU擅长的`int8`定点运算（低消耗），这是在边缘端部署AI的**关键复杂度优化**。
  - **NPU执行：** 当我们调用`rknn_run()`时，模型的计算复杂度（例如 5 G-FLOPs）被NPU的硬件算力（1.0 TOPS）承接，CPU仅需等待其完成。
  - **CPU (对照组)：** 在“纯软”路径中，这 5 G-FLOPs 的计算量**全部由ARM A55 CPU承担**，导致其帧率极低，验证了NPU的优越性。
- **3. 后处理 (CPU) 复杂度：O(M²)**
  - **挑战：** NPU的输出是海量的候选框（例如YOLOv5的25200个）和置信度。CPU必须执行“后处理”（解码、过滤、NMS）才能得到最终结果。
  - **NMS (非极大值抑制)：** 这是后处理中最耗时的步骤。其朴素算法复杂度为**O(M²)**（M为候选框数量）。
  - **我们的方案：** 我们在CPU上实现了高效的后处理逻辑。首先，通过**置信度阈值**（用户可调）过滤掉绝大多数低质量的候选框，将 M 的数量级从几万降低到几十或几百。然后，再对剩余的M个框执行NMS。这一“先过滤，再NMS”的策略，极大地降低了实际运行的复杂度，确保了后处理不会成为CPU的瓶颈。

## 第三部分：系统架构详解

### 3.1 硬件系统：高速“采集-处理-传输”流水线

我们的硬件系统（FPGA）构建了一个并行的写入路径和读出路径的双工数据流，以DDR3作为中央帧缓存。

> **【建议您在此处插入报告中的 ‘图 硬件系统结构框图’】**

- **1. 写入路径 (并行ISP)：**
  - 三路CMOS摄像头并行采集数据（`cmos1_data`, `cmos2_data`, `cmos3_data`）。
  - 每路数据流依次经过：
    1. `cmos_8_16bit`：数据位宽转换。
    2. `rgb_to_filtered_gray16`：**片上ISP核心**，执行灰度化与3x3中值滤波。
    3. `reshape`：降采样与数据准备。
  - 处理后的四路数据（一路被复制）被分配到4个独立的AXI通道。
  - `4通道AXI内存控制器`：作为交通枢纽，将四路数据仲裁后写入`DDR3 IP core`。
- **2. 读出路径 (PCIe高速传输)：**
  - `PCIe DMA`（由ARM端驱动）触发读取请求。
  - `AXI内存控制器`从DDR3中并行读回4个通道的数据。
  - `pcie_image_select`：**四象限拼接器**。它将四路640x360的数据拼接成一个128bit的1280x720四宫格图像流。
  - `ips2l_pcie_dma`：DMA引擎，将拼接后的图像流打包成AXI-Stream TLP包。
  - `pcie_test` (PCIe IP核)：物理IP核，通过`txn/txp`接口将数据包高速发送至ARM主机内存。

这一“写-读”双工架构，解耦了采集端和处理端，DDR3帧缓存起到了至关重要的数据缓冲作用，确保了FPGA和ARM之间数据流的稳定与高速。

### 3.2 软件系统：多线程动态调度架构

我们的软件系统（ARM端）是一个基于C++/Qt 5.12.9的多线程应用，其核心是高内聚、低耦合的**七线程模型**，确保了UI的绝对流畅和后端处理的最高效率。

> **【建议您在此处插入报告中的 ‘图 系统整体框图’】**

- **1. 主线程 (UI 线程):**
  - **职责：** 负责 `main.cpp` 启动、QML界面加载与渲染、所有用户交互（按钮点击、滑块拖动）的响应。
  - **核心：** 充当所有后台线程的“指挥官”，通过Qt的**信号槽机制**分发任务和回收结果。
- **2. 采集线程 (Capture Thread):**
  - **职责：** 运行 `CaptureDevice` 对象。专职通过 `ioctl` 与 `/dev/pango_pci_driver` 驱动通信，从FPGA的PCIe总线中高速拉取视频数据流。
  - **硬协同：** 拉取到RGB565数据后，立即调用**RGA硬件**进行第一次色彩空间转换（转为RGB888），为CPU减负。
- **3. NPU-YOLOv5 推理线程:**
  - **职责：** 运行 `NPUProcessor` 对象。负责完整的“硬”协同路径：
    1. [硬: RGA] 硬件预处理 (缩放、Letterbox、RGB->BGR)。
    2. [硬: NPU] `rknn_run()` 执行推理。
    3. [软: CPU] `postprocess_yolov5` 执行后处理和NMS。
    4. [软: CPU] `updateTracker` 执行目标跟踪与告警逻辑。
- **4. NPU-YOLOv6 推理线程:**
  - **职责：** 运行 `YOLOv6Processor` 对象。流水线与YOLOv5类似，但其后处理逻辑（`postprocess_yolov6`）完全不同，用于解码DFLv6的特定输出。
- **5. NPU-Mobilenet 推理线程:**
  - **职责：** 运行 `MobilenetProcessor` 对象。负责图像分类任务，其后处理为`Top-K`排序。
- **6. CPU-YOLOv5 推理线程 (对照组):**
  - **职责：** 运行 `CPUProcessor` 对象。全程“纯软”：
    1. [软: CPU] OpenCV执行预处理。
    2. [软: CPU] `cv::dnn::Net::forward()` 执行推理。
    3. [软: CPU] `detect()` 执行后处理。
- **7. 上传线程 (Upload Thread):**
  - **职责：** 运行 `SdkUploader` 对象。此线程专职处理所有**阻塞式网络I/O**。
  - **协同：** 当AI线程触发告警时，主线程将任务派发至此。该线程独立执行`Qiniu_Io_PutFile`（上传七牛云）和`sendMiaoTiXingAlert`（推送提醒）。
  - **关键设计：** 这种**异步设计**确保了AI推理线程和主UI线程**永远不会**因为网络延迟（可能长达数秒）而被阻塞。
- **核心防崩溃机制 (反压):** 为防止AI处理速度跟不上FPGA采集速度（25fps）而导致内存溢出，我们设计了`std::atomic<bool> m_processor_is_ready`标志。当采集线程发来新一帧数据时，主控`VideoWindow`会检查此标志：
  1. 若为 `false` (AI线程正忙)，则**主动丢弃**此帧。
  2. 若为 `true` (AI线程空闲)，则立刻置为 `false` (上锁)，并将帧派发给AI线程。
  3. AI线程处理完毕后，发回信号，主控再将标志置为 `true` (解锁)。 这种“反压”机制确保了系统始终在高负载下稳定运行。

## 第四部分：性能指标与成果展示

### 4.1 核心性能指标

| 类别     | 指标                                          | 备注                           |
| -------- | --------------------------------------------- | ------------------------------ |
| 核心平台 | 紫光同创 RK3568_MES2L50H                      | FPGA (PG2L50H) + ARM (RK3568)  |
| NPU算力  | 1.0 TOPS                                      | 支持`int8`高效推理             |
| 视频输入 | 3路OV5640并行输入                             | 经FPGA拼接为伪四路1280x720输出 |
| 硬件ISP  | 灰度化, 3x3中值滤波                           | FPGA片上实时完成，0-CPU占用    |
| 显示帧率 | 采集图像 (25 fps) / AI推理 (20 fps)           | 采集与AI异步并行处理           |
| AI模型   | 4种 (NPU: YOLOv5, v6, Mobilenet; CPU: YOLOv5) | 展示了算法多样性与异构路径     |
| 识别种类 | 80种 (YOLO) / 1000种 (MobileNet)              |                                |
| 可调参数 | 置信度、NMS阈值、报警时间、MobileNet Top-K    | QML界面实时动态调参            |
| 协同架构 | 边缘决策 + 异步云端告警                       | 七牛云KODO + 喵提醒API         |

### 4.2 成果展示：动态可交互QML界面

我们的软件系统在RK3568的Linux发行版上运行，最终成果是一个高性能、多线程、可交互的智慧视觉应用。

- **视频显示区 (左侧/中间):** 左侧`FPGA Camera View`显示FPGA通过PCIe传来的原始（已拼接）图像流。中间`AI Inference View`显示AI处理器处理后的结果，实时绘制跟踪框或分类结果。
- **异构模型调度区 (底部):** 核心交互功能。用户点击按钮，`VideoWindow`主控器便会更改状态机，将数据流动态派发至不同的AI线程（NPU或CPU）。
- **AI参数实时调节区 (右侧):** 系统的“高级交互”体现。当用户拖动`置信度(Conf)`滑块时，信号会**广播**给所有YOLO处理器，使其立即应用新的阈值，实现AI算法的**在线调优**。

## 第五部分：总结与展望

### 5.1 总结

本项目从大赛的“软硬协同”要求出发，完整设计并实现了一套从FPGA硬件ISP流水线、DDR3帧缓存、PCIe高速传输，到ARM端多线程异构（NPU/CPU/RGA）AI推理，最后到异步云端告警的**全链路“智慧Vision”系统**。

我们通过“硬”预处理、“硬”AI加速、“软”灵活调度，实现了对RK3568_MES2L50H平台算力的深度压榨。系统所展现的**低延迟边缘决策**、**高可靠异步告警**、**高灵活性多模型调度**，使其在智能安防、工业安全检测等领域具有广泛的应用潜力。

### 5.2 可扩展之处

- **帧率提升：** 当前的PCIe传输是逐行读取拼帧，未来可修改AXI控制器和DMA逻辑，实现整帧突发传输，预计可将720p帧率提升至40-50fps。
- **分辨率提升：** 修改CMOS寄存器配置，将采集分辨率从720p提升至1080p，为AI提供更丰富的细节。
- **架构升级 (多路并发)：** 当前为多路“拼接”后单路AI处理。未来可升级软件架构，将4路640x360数据流**并行送入4个独立的AI推理线程**，实现真正的多路并发AI分析，进一步提高识别精度。
- **系统配置UI化：** 允许用户通过UI界面动态修改和保存AI模型路径和云端密钥，使软件脱离编译环境，成为可分发的商业产品。
